{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05cdc157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Needed for a bit more reproducibility of results when using TensorFlow\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea2177c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 17:52:13.399934: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1758469935.066820  178369 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9171 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-b0_notop.h5\n",
      "\u001b[1m24274472/24274472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from experiments_config import (\n",
    "    CommonConfig,\n",
    "    MlflowConfig,\n",
    "    DatasetConfig,\n",
    "    ModelConfig,\n",
    "    ModelTrainingConfig,\n",
    "    ModelEvaluationConfig,\n",
    ")\n",
    "\n",
    "common_config = CommonConfig()\n",
    "mlflow_config = MlflowConfig()\n",
    "dataset_config = DatasetConfig()\n",
    "model_config = ModelConfig()\n",
    "training_config = ModelTrainingConfig()\n",
    "evaluation_config = ModelEvaluationConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ee96ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log system metrics in the mlflow server\n",
    "if mlflow_config.ENABLE_SYSTEM_METRICS_LOGGING:\n",
    "    mlflow.enable_system_metrics_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05f64ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/2', creation_time=1758296474574, experiment_id='2', last_update_time=1758296474574, lifecycle_stage='active', name='Smart_Recycling_AI', tags={'dataset': 'garbage-dataset-v1',\n",
       " 'framework': 'tensorflow-keras',\n",
       " 'mlflow.experimentKind': 'custom_model_development',\n",
       " 'mlflow.note.content': ' This experiment focuses on Smart Recycling using '\n",
       "                        'computer vision. \\n'\n",
       "                        '    The goal is to classify waste items into 8 '\n",
       "                        'categories: battery, biological, clothes, glass, '\n",
       "                        'metal, paper, plastic, and trash. \\n'\n",
       "                        '    Different model architectures and augmentation '\n",
       "                        'strategies are tested to evaluate their performance '\n",
       "                        'and identify the best approach \\n'\n",
       "                        '    for robust and scalable recycling classification.',\n",
       " 'num_classes': '8',\n",
       " 'project_name': 'smart-recycling',\n",
       " 'status': 'in-progress',\n",
       " 'task': 'image-classification',\n",
       " 'team': 'ai-team-xyz'}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the tracking URI and experiment for subsequent runs\n",
    "\n",
    "mlflow.set_tracking_uri(mlflow_config.MLFLOW_TRACKING_URI)\n",
    "mlflow.set_experiment(mlflow_config.MLFLOW_EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fe70467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a custom run name for better identification in the MLflow UI\n",
    "mlflow.set_tag(\"mlflow.runName\", mlflow_config.MLFLOW_RUN_NAME)\n",
    "\n",
    "# set the dataset as tag in the mlflow run\n",
    "mlflow.set_tag(\"dataset\", dataset_config.DATASET)\n",
    "\n",
    "# set a description for the MLflow run\n",
    "mlflow.set_tag(\"mlflow.note.content\", mlflow_config.MLFLOW_RUN_DESCRIPTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dd9999e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "\n",
    "if mlflow_config.MLFLOW_LOG_GIT_SHA:\n",
    "    repo = git.Repo(search_parent_directories=True)\n",
    "    sha = repo.head.object.hexsha\n",
    "    # set the git commit sha as a tag in the mlflow run for better traceability\n",
    "    mlflow.set_tag(\"git_commit\", sha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf254744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log the experiments_config.py for future auditability\n",
    "mlflow.log_artifact(\n",
    "    common_config.PATH_TO_CONFIG_FILE,\n",
    "    artifact_path=\"config.py\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed0c4ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14819 files belonging to 8 classes.\n",
      "Found 1973 files belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "from smart_recycling.utils import get_tensorflow_dataset\n",
    "\n",
    "# Tensorflow Dataset loading\n",
    "\n",
    "train_dataset = get_tensorflow_dataset(\n",
    "    image_folder=f\"{dataset_config.DATASET_FOLDER}/train\",\n",
    "    image_size=dataset_config.IMAGE_SIZE,\n",
    "    batch_size=dataset_config.TRAIN_BATCH_SIZE,\n",
    "    label_mode=dataset_config.LABEL_MODE,\n",
    "    shuffle=True,  # shuffle True for training dataset\n",
    "    seed=common_config.SEED,\n",
    ")\n",
    "\n",
    "val_dataset = get_tensorflow_dataset(\n",
    "    image_folder=f\"{dataset_config.DATASET_FOLDER}/val\",\n",
    "    image_size=dataset_config.IMAGE_SIZE,\n",
    "    batch_size=dataset_config.VALIDATION_BATCH_SIZE,\n",
    "    label_mode=dataset_config.LABEL_MODE,\n",
    "    shuffle=False,  # shuffle False for validation dataset\n",
    "    seed=common_config.SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e49fd5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# setup mixed precision if wanted\n",
    "# Reference: https://www.tensorflow.org/guide/mixed_precision\n",
    "if model_config.ENABLE_MIXED_PRECISION:\n",
    "    tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "model = model_config.MODEL\n",
    "\n",
    "model.compile(\n",
    "    optimizer=model_config.OPTIMIZER,\n",
    "    loss=model_config.LOSS,\n",
    "    metrics=model_config.METRICS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67df7286",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 17:52:23.691972: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "from smart_recycling.utils import compute_class_weights, get_true_labels\n",
    "\n",
    "class_weight = None\n",
    "\n",
    "# Optionally compute class weights to handle class imbalance\n",
    "if training_config.COMPUTE_CLASS_WEIGHTS:\n",
    "    y_true = get_true_labels(train_dataset)\n",
    "    class_weight = compute_class_weights(\n",
    "        y_true, class_weight=training_config.CLASS_WEIGHTING_METHOD\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d2f4155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 17:52:30.900450: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 71ms/step - f1_score: 0.8242 - loss: 0.7100 - val_f1_score: 0.9354 - val_loss: 0.2524 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 60ms/step - f1_score: 0.8932 - loss: 0.3786 - val_f1_score: 0.9424 - val_loss: 0.2029 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 61ms/step - f1_score: 0.9072 - loss: 0.3152 - val_f1_score: 0.9498 - val_loss: 0.1776 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 61ms/step - f1_score: 0.9163 - loss: 0.2895 - val_f1_score: 0.9521 - val_loss: 0.1670 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 60ms/step - f1_score: 0.9192 - loss: 0.2766 - val_f1_score: 0.9487 - val_loss: 0.1637 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 61ms/step - f1_score: 0.9219 - loss: 0.2562 - val_f1_score: 0.9536 - val_loss: 0.1580 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 61ms/step - f1_score: 0.9271 - loss: 0.2437 - val_f1_score: 0.9527 - val_loss: 0.1562 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 61ms/step - f1_score: 0.9293 - loss: 0.2300 - val_f1_score: 0.9527 - val_loss: 0.1531 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 61ms/step - f1_score: 0.9280 - loss: 0.2243 - val_f1_score: 0.9552 - val_loss: 0.1476 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 61ms/step - f1_score: 0.9308 - loss: 0.2189 - val_f1_score: 0.9552 - val_loss: 0.1470 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 61ms/step - f1_score: 0.9342 - loss: 0.2109 - val_f1_score: 0.9567 - val_loss: 0.1433 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 61ms/step - f1_score: 0.9385 - loss: 0.1998 - val_f1_score: 0.9566 - val_loss: 0.1393 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 61ms/step - f1_score: 0.9410 - loss: 0.1953 - val_f1_score: 0.9582 - val_loss: 0.1410 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 60ms/step - f1_score: 0.9403 - loss: 0.1912 - val_f1_score: 0.9586 - val_loss: 0.1393 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 60ms/step - f1_score: 0.9406 - loss: 0.1892 - val_f1_score: 0.9581 - val_loss: 0.1358 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 61ms/step - f1_score: 0.9436 - loss: 0.1811 - val_f1_score: 0.9568 - val_loss: 0.1396 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 61ms/step - f1_score: 0.9445 - loss: 0.1780 - val_f1_score: 0.9561 - val_loss: 0.1369 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 61ms/step - f1_score: 0.9450 - loss: 0.1771 - val_f1_score: 0.9581 - val_loss: 0.1314 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 60ms/step - f1_score: 0.9470 - loss: 0.1702 - val_f1_score: 0.9586 - val_loss: 0.1314 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 61ms/step - f1_score: 0.9460 - loss: 0.1728 - val_f1_score: 0.9592 - val_loss: 0.1261 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 61ms/step - f1_score: 0.9469 - loss: 0.1704 - val_f1_score: 0.9595 - val_loss: 0.1303 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 61ms/step - f1_score: 0.9449 - loss: 0.1749 - val_f1_score: 0.9571 - val_loss: 0.1381 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 61ms/step - f1_score: 0.9462 - loss: 0.1647 - val_f1_score: 0.9586 - val_loss: 0.1288 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 61ms/step - f1_score: 0.9480 - loss: 0.1601 - val_f1_score: 0.9570 - val_loss: 0.1307 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 61ms/step - f1_score: 0.9504 - loss: 0.1571 - val_f1_score: 0.9591 - val_loss: 0.1294 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 61ms/step - f1_score: 0.9508 - loss: 0.1515 - val_f1_score: 0.9610 - val_loss: 0.1265 - learning_rate: 1.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 60ms/step - f1_score: 0.9525 - loss: 0.1486 - val_f1_score: 0.9605 - val_loss: 0.1271 - learning_rate: 1.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 61ms/step - f1_score: 0.9501 - loss: 0.1526 - val_f1_score: 0.9600 - val_loss: 0.1265 - learning_rate: 1.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 60ms/step - f1_score: 0.9518 - loss: 0.1521 - val_f1_score: 0.9590 - val_loss: 0.1262 - learning_rate: 1.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 60ms/step - f1_score: 0.9501 - loss: 0.1586 - val_f1_score: 0.9595 - val_loss: 0.1261 - learning_rate: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "# Actual train the model\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=training_config.EPOCHS,\n",
    "    callbacks=training_config.TRAINING_CALLBACKS,\n",
    "    class_weight=class_weight,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "372a2b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:The `save_format` argument is deprecated in Keras 3. We recommend removing this argument as it can be inferred from the file path. Received: save_format=keras\n",
      "2025/09/21 17:59:47 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n"
     ]
    }
   ],
   "source": [
    "# Optional log the model\n",
    "\n",
    "if mlflow_config.MLFLOW_LOG_MODEL:\n",
    "    # get one batch of images to use as input example to infer the signature for\n",
    "    # logging the model\n",
    "    for x, y in train_dataset.take(1):\n",
    "        input_example = x[:1].numpy()\n",
    "        break\n",
    "\n",
    "    mlflow.tensorflow.log_model(\n",
    "        model, **mlflow_config.MLFLOW_LOG_MODEL_CONFIG, input_example=input_example\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e841295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2970 files belonging to 8 classes.\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - f1_score: 0.9503 - loss: 0.1656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 17:59:58.133206: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "from smart_recycling.utils import (\n",
    "    save_prediction_time,\n",
    "    save_confusion_matrix,\n",
    "    save_prediction_csv,\n",
    "    save_model_history,\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "# Optional evaluate the model on the test set\n",
    "if evaluation_config.INCLUDE_EVALUATION_ON_TEST_SET:\n",
    "    # load the test dataset\n",
    "    test_dataset = get_tensorflow_dataset(\n",
    "        image_folder=f\"{dataset_config.DATASET_FOLDER}/test\",\n",
    "        image_size=dataset_config.IMAGE_SIZE,\n",
    "        batch_size=dataset_config.TEST_BATCH_SIZE,\n",
    "        label_mode=dataset_config.LABEL_MODE,\n",
    "        shuffle=False,  # shuffle needs to be false for later evaluation\n",
    "        seed=common_config.SEED,\n",
    "    )\n",
    "\n",
    "    test_results = model.evaluate(test_dataset, return_dict=True)\n",
    "    # log the test results to mlflow with a \"test_\" prefix\n",
    "    for name, value in test_results.items():\n",
    "        mlflow.log_metric(f\"test_{name}\", value)\n",
    "\n",
    "    if evaluation_config.SAVE_MODEL_HISTORY:\n",
    "        save_model_history(history)\n",
    "\n",
    "    # optional save the prediction time to mlflow (in milliseconds)\n",
    "    if evaluation_config.SAVE_PREDICTION_TIME:\n",
    "        y_probs = save_prediction_time(model, test_dataset)\n",
    "\n",
    "    # if confusion matrix or prediction csv should be saved, we need the predicted and true labels\n",
    "    # additional the file paths and class names are needed\n",
    "    if evaluation_config.SAVE_CONFUSION_MATRIX or evaluation_config.SAVE_PREDICTION_CSV:\n",
    "        y_pred = np.argmax(y_probs, axis=1)\n",
    "\n",
    "        y_true = get_true_labels(test_dataset)\n",
    "\n",
    "        file_paths = test_dataset.file_paths\n",
    "        class_names = test_dataset.class_names\n",
    "\n",
    "    # optional save the confusion matrix to mlflow as plot\n",
    "    if evaluation_config.SAVE_CONFUSION_MATRIX:\n",
    "        save_confusion_matrix(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            class_names,\n",
    "        )\n",
    "\n",
    "    # optional save two csv, one with all predictions and one with missclassified samples\n",
    "    if evaluation_config.SAVE_PREDICTION_CSV:\n",
    "        save_prediction_csv(file_paths, y_true, y_pred, y_probs, class_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smart_recycling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
